Reinforcement Learning

Suitable for learning from interactive platforms.
Learns from experience with no prior dataset

Doesn't uncover structure like Unsupervised Learning, but rather works on maximising the reward signal

Two important Es in Reinforcement Learning
The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make
better action selections in the future
Neither can be performed exclusively without failure

All reinforcement learning agents have explicit goals, can
sense aspects of their environments, and can choose actions to influence their environments.

Now by a complete, interactive, goal-seeking agent we do not always mean something
like a complete organism or robot. These are clearly examples, but a complete,
interactive, goal-seeking agent can also be a component of a larger behaving system.
In this case, the agent directly interacts with the rest of the larger system and indirectly
interacts with the larger system’s environment. A simple example is an agent
that monitors the charge level of robot’s battery and sends commands to the robot’s
control architecture. This agent’s environment is the rest of the robot together
with the robot’s environment. One must look beyond the most obvious examples
of agents and their environments to appreciate the generality of the reinforcement
learning framework.


Examples:

A master chess player makes a move. The choice is informed both by planning—
anticipating possible replies and counterreplies—and by immediate, intuitive
judgments of the desirability of particular positions and moves.

A gazelle calf struggles to its feet minutes after being born. Half an hour later
it is running at 20 miles per hour.

Elements:
Beyond the agent and the environment, one can identify four main subelements of
a reinforcement learning system: a policy, a reward signal, a value function, and,
optionally, a model of the environment.

A policy defines the learning agent’s way of behaving at a given time. Roughly
speaking, a policy is a mapping from perceived states of the environment to actions to
be taken when in those states.
In general, policies may be stochastic(randomly determined)

A reward signal defines the goal in a reinforcement learning problem. On each time
step, the environment sends to the reinforcement learning agent a single number, a
reward. The agent’s sole objective is to maximize the total reward it receives over
the long run. The reward signal thus defines what are the good and bad events for
the agent.

Whereas the reward signal indicates what is good in an immediate sense, a value
function specifies what is good in the long run. Roughly speaking, the value of
a state is the total amount of reward an agent can expect to accumulate over the
future, starting from that state.

Rewards are in a sense primary, whereas values, as predictions of rewards, are
secondary. Without rewards there could be no values, and the only purpose of estimating
values is to achieve more reward. Nevertheless, it is values with which we
are most concerned when making and evaluating decisions. Action choices are made
based on value judgments.

A model of the environment is something that mimics the behavior of the environment,
or more generally, that allows inferences to be made about how the environment will
behave. For example, given a state and action, the model might predict the resultant
next state and next reward.

Methods for solving reinforcement learning problems
that use models and planning are called model-based methods, as opposed to simpler
model-free methods that are explicitly trial-and-error learners—viewed as almost the
opposite of planning.

Limitations and Scope:

If the space of policies is sufficiently small, or can be structured
so that good policies are common or easy to find—or if a lot of time is available for
the search—then evolutionary methods can be effective. In addition, evolutionary
methods have advantages on problems in which the learning agent cannot accurately
sense the state of its environment.

When we say that a reinforcement learning agent’s goal is to maximize a numerical reward
signal, we of course are not insisting that the agent has to actually achieve the goal of maximum reward.

Formula:

V (s) ← V (s) + α[V(s')-V(s)]

where α is a small positive fraction called the step-size parameter, which influences
the rate of learning. This update rule is an example of a temporal-difference learning
method, so called because its changes are based on a difference, V (s')−V (s), between
estimates at two different times.

To evaluate a policy an evolutionary method
holds the policy fixed and plays many games against the opponent, or simulates
many games using a model of the opponent. The frequency of wins gives an unbiased
estimate of the probability of winning with that policy, and can be used to direct
the next policy selection. But each policy change is made only after many games,
and only the final outcome of each game is used: what happens during the games
is ignored. For example, if the player wins, then all of its behavior in the game is
given credit, independently of how specific moves might have been critical to the
win. Credit is even given to moves that never occurred! Value function methods, in
contrast, allow individual states to be evaluated. In the end, evolutionary and value
function methods both search the space of policies, but learning a value function
takes advantage of information available during the course of play.

Some neural-network textbooks have used the term “trial-and-error” to
describe networks that learn from training examples. This is an understandable confusion
because these networks use error information to update connection weights,
but this misses the essential character of trial-and-error learning as selecting actions
on the basis of evaluative feedback that does not rely on knowledge of what the
correct action should be.
